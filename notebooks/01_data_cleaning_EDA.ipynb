{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee8dd08",
   "metadata": {},
   "source": [
    "# 01 Data Cleaning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f507f2",
   "metadata": {},
   "source": [
    "Phase 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd054b0d",
   "metadata": {},
   "source": [
    "Phase 1\t\n",
    "\t1.\tCompute within / between variance of ND-GAIN at annual frequency (total, between, within, share within). Save a small table and histogram of year-to-year changes (diff1) and report per-country within SD.\n",
    "\t2.\tCount how many countries and how many country-years show substantive ND-GAIN change; identify outliers (big diffs).\n",
    "\t3.\tCompute per-country summary (mean, SD, number of years). Produce a short paragraph interpreting whether FE is likely to be feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c492d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18880091",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82931855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance decomposition of ND-GAIN (annual):\n",
      "  Total variance:   116.0202\n",
      "  Between variance: 114.1825\n",
      "  Within variance:  4.5763\n",
      "  Share within:     3.94%\n",
      "\n",
      "Number of country-years with substantive ND-GAIN change (>|0.98|): 178\n",
      "Number of countries with at least one substantive change: 57\n",
      "\n",
      "Top 5 outlier year-to-year changes in ND-GAIN:\n",
      "     iso3c  year  gain_diff1\n",
      "2228   BGD  2014  -11.062157\n",
      "2274   NGA  2014  -10.871986\n",
      "2256   IND  2014   -9.909187\n",
      "2278   PAK  2014   -8.339038\n",
      "2270   MEX  2014   -7.692216\n",
      "\n",
      "Interpretation:\n",
      "The variance decomposition shows that a substantial share of the total variance in ND-GAIN is due to between-country differences, with within-country (over time) variance being relatively smaller. The per-country within SDs are generally low, and only a small number of country-years show substantive changes in ND-GAIN from year to year. This suggests that country fixed effects (FE) models may be feasible, but the limited within-country variation could reduce the power to detect effects of time-varying covariates. Outlier years with large changes should be checked for data quality or exceptional events.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/leosgambato/Documents/GitHub/Capstone/data/processed/baseline_with_gain.csv')\n",
    "\n",
    "# Clean up: drop rows where 'gain' is missing or iso3c is missing\n",
    "df = df.dropna(subset=['gain', 'iso3c','sovereign_spread'])\n",
    "\n",
    "# 1. Compute within / between variance of Gain at annual frequency\n",
    "\n",
    "# Compute overall mean\n",
    "overall_mean = df['gain'].mean()\n",
    "\n",
    "# Compute per-country mean\n",
    "country_means = df.groupby('iso3c')['gain'].mean()\n",
    "\n",
    "# Merge country means back to df\n",
    "df = df.merge(country_means.rename('country_mean'), left_on='iso3c', right_index=True)\n",
    "\n",
    "# Between variance: variance of country means\n",
    "between_var = country_means.var(ddof=0)\n",
    "\n",
    "# Within variance: mean of per-country variances\n",
    "within_var = df.groupby('iso3c')['gain'].var(ddof=0).mean()\n",
    "\n",
    "# Total variance\n",
    "total_var = df['gain'].var(ddof=0)\n",
    "\n",
    "# Share within\n",
    "share_within = within_var / total_var if total_var > 0 else np.nan\n",
    "\n",
    "print(\"Variance decomposition of ND-GAIN (annual):\")\n",
    "print(f\"  Total variance:   {total_var:.4f}\")\n",
    "print(f\"  Between variance: {between_var:.4f}\")\n",
    "print(f\"  Within variance:  {within_var:.4f}\")\n",
    "print(f\"  Share within:     {share_within:.2%}\")\n",
    "\n",
    "# 2. Histogram and table of year-to-year changes (diff1), per-country within SD\n",
    "\n",
    "# Sort for diff calculation\n",
    "df = df.sort_values(['iso3c', 'year'])\n",
    "\n",
    "# Compute year-to-year difference\n",
    "df['gain_diff1'] = df.groupby('iso3c')['gain'].diff()\n",
    "\n",
    "# Table of year-to-year changes (drop NA)\n",
    "diff1_table = df[['iso3c', 'year', 'gain_diff1']].dropna()\n",
    "\n",
    "# Save small table (first 10 rows as example)\n",
    "diff1_table.head(10).to_csv('/Users/leosgambato/Documents/GitHub/Capstone/outputs/gain_diff1_sample.csv', index=False)\n",
    "\n",
    "# Histogram of all year-to-year changes\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(diff1_table['gain_diff1'], bins=30, edgecolor='k')\n",
    "plt.title('Histogram of Year-to-Year Changes in ND-GAIN')\n",
    "plt.xlabel('Year-to-Year Change (diff1)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/leosgambato/Documents/GitHub/Capstone/outputs/gain_diff1_hist.png')\n",
    "plt.close()\n",
    "\n",
    "# Per-country within SD\n",
    "country_within_sd = df.groupby('iso3c')['gain'].std().rename('within_sd')\n",
    "country_within_sd = country_within_sd.reset_index()\n",
    "country_within_sd.head(10).to_csv('/Users/leosgambato/Documents/GitHub/Capstone/outputs/gain_within_sd_sample.csv', index=False)\n",
    "\n",
    "# 2. Count countries and country-years with substantive ND-GAIN change; identify outliers\n",
    "\n",
    "# Define substantive change threshold (e.g., > 1 SD of all diffs)\n",
    "diff1_sd = diff1_table['gain_diff1'].std()\n",
    "substantive_thresh = diff1_sd\n",
    "\n",
    "# Count country-years with |diff1| > threshold\n",
    "substantive_changes = diff1_table[np.abs(diff1_table['gain_diff1']) > substantive_thresh]\n",
    "n_substantive = len(substantive_changes)\n",
    "n_countries = substantive_changes['iso3c'].nunique()\n",
    "\n",
    "print(f\"\\nNumber of country-years with substantive ND-GAIN change (>|{substantive_thresh:.2f}|): {n_substantive}\")\n",
    "print(f\"Number of countries with at least one substantive change: {n_countries}\")\n",
    "\n",
    "# Identify outliers (e.g., top 5 biggest absolute diffs)\n",
    "outliers = diff1_table.reindex(diff1_table['gain_diff1'].abs().sort_values(ascending=False).index).head(5)\n",
    "print(\"\\nTop 5 outlier year-to-year changes in ND-GAIN:\")\n",
    "print(outliers)\n",
    "\n",
    "# 3. Per-country summary (mean, SD, number of years)\n",
    "country_summary = df.groupby('iso3c').agg(\n",
    "    mean_gain=('gain', 'mean'),\n",
    "    sd_gain=('gain', 'std'),\n",
    "    n_years=('gain', 'count')\n",
    ").reset_index()\n",
    "\n",
    "country_summary.head(10).to_csv('/Users/leosgambato/Documents/GitHub/Capstone/outputs/gain_country_summary_sample.csv', index=False)\n",
    "\n",
    "# Short paragraph interpreting FE feasibility\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"The variance decomposition shows that a substantial share of the total variance in ND-GAIN is due to between-country differences, with within-country (over time) variance being relatively smaller. The per-country within SDs are generally low, and only a small number of country-years show substantive changes in ND-GAIN from year to year. This suggests that country fixed effects (FE) models may be feasible, but the limited within-country variation could reduce the power to detect effects of time-varying covariates. Outlier years with large changes should be checked for data quality or exceptional events.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889a28e",
   "metadata": {},
   "source": [
    "### Data cleaning, Exploratory analysis, creating lags etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc51f909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values summary:\n",
      "cpi_yoy                   267\n",
      "wgi_cc                    144\n",
      "wgi_ge                    144\n",
      "wgi_pv                    144\n",
      "wgi_rl                    144\n",
      "wgi_rq                    144\n",
      "wgi_va                    144\n",
      "gain_diff1                 67\n",
      "gdp_annual_growth_rate     66\n",
      "debt_to_gdp                11\n",
      "dtype: int64\n",
      "\n",
      "Dataset ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining missing values\n",
    "print(f\"\\nMissing values summary:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "if len(missing_summary) > 0:\n",
    "    print(missing_summary)\n",
    "else:\n",
    "    print(\"No missing values in the dataset\")\n",
    "\n",
    "print(\"\\nDataset ready for analysis!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5efa82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "965b1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure year is integer\n",
    "df['year'] = df['year'].astype(int)\n",
    "df = df.sort_values(['iso3c','year']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ded51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transforms (avoid log(0) by replacing zeros with tiny positive number)\n",
    "df['ln_gdp_per_capita'] = np.log(df['gdp_per_capita'].replace(0, np.nan))\n",
    "df['ln_gross_gdp'] = np.log(df['gross gdp'].replace(0, np.nan))\n",
    "\n",
    "# If you prefer to drop the raw gross_gdp or gdp_per_capita from covariates, decide later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388cb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lags(df, group='iso3c', time='year', variables=None, lags=(1,2)):\n",
    "    \"\"\"\n",
    "    Add lagged versions of `variables` grouped by `group` and sorted by `time`.\n",
    "    variables: list of column names\n",
    "    lags: tuple/list of lag integers e.g. (1,2)\n",
    "    \"\"\"\n",
    "    df = df.sort_values([group, time]).copy()\n",
    "    for var in variables:\n",
    "        for lag in lags:\n",
    "            new_col = f\"{var}_lag{lag}\"\n",
    "            df[new_col] = df.groupby(group)[var].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Variables we recommend lagging (one or two lags)\n",
    "vars_to_lag = [\n",
    "    'sovereign_spread',    # outcome lag Y_lag1\n",
    "    'cpi_yoy',\n",
    "    'gdp_annual_growth_rate',\n",
    "    'ln_gdp_per_capita',\n",
    "    'ln_gross_gdp',\n",
    "    'debt_to_gdp',\n",
    "    'deficit_to_gdp',\n",
    "    'current_account_balance',\n",
    "    # Add global factors if present (VIX, US10y, Brent)\n",
    "    # 'VIX', 'US10y', 'Brent'\n",
    "]\n",
    "\n",
    "# add lag1 and lag2 (you can restrict to only lag1 by passing lags=(1,))\n",
    "df = create_lags(df, group='iso3c', time='year', variables=vars_to_lag, lags=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07643b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ND-GAIN is 'gain' column\n",
    "df = df.sort_values(['iso3c','year']).reset_index(drop=True)\n",
    "df['gain_diff1'] = df.groupby('iso3c')['gain'].diff(1)\n",
    "df['gain_diff3'] = df.groupby('iso3c')['gain'].diff(3)\n",
    "df['gain_diff5'] = df.groupby('iso3c')['gain'].diff(5)\n",
    "\n",
    "# outlier threshold (you used 0.91 earlier; keep parametric)\n",
    "OUTLIER_THRESHOLD = 0.91\n",
    "df['gain_substantive_change'] = df['gain_diff1'].abs() > OUTLIER_THRESHOLD\n",
    "\n",
    "# list top K absolute diffs for quick inspection\n",
    "def top_gain_changes(df, k=10):\n",
    "    tmp = df[['iso3c','year','gain_diff1']].dropna().assign(absdiff= df['gain_diff1'].abs())\n",
    "    return tmp.sort_values('absdiff', ascending=False).head(k)\n",
    "\n",
    "# winsorize function\n",
    "def winsorize_ser(s, lower_pct=0.01, upper_pct=0.99):\n",
    "    lo = s.quantile(lower_pct)\n",
    "    hi = s.quantile(upper_pct)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "# Example: create winsorized diff for diagnostics (do not replace original unless you choose)\n",
    "df['gain_diff1_wins'] = winsorize_ser(df['gain_diff1'], 0.01, 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4c1b2",
   "metadata": {},
   "source": [
    "#### 5) Imputation flags (per variable) — create columns marking missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45fc2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose list of candidate covariates we'll eventually impute (lagged and level vars)\n",
    "candidate_covs = [\n",
    "    # lagged covariates\n",
    "    'sovereign_spread_lag1', 'sovereign_spread_lag2',\n",
    "    'cpi_yoy_lag1', 'cpi_yoy_lag2',\n",
    "    'gdp_annual_growth_rate_lag1', 'gdp_annual_growth_rate_lag2',\n",
    "    'ln_gdp_per_capita_lag1', 'ln_gdp_per_capita_lag2',\n",
    "    'debt_to_gdp_lag1', 'debt_to_gdp_lag2',\n",
    "    'deficit_to_gdp_lag1', 'deficit_to_gdp_lag2',\n",
    "    'current_account_balance_lag1', 'current_account_balance_lag2',\n",
    "    # governance (we treat as slow-moving; missing flags still useful)\n",
    "    'wgi_cc','wgi_ge','wgi_pv','wgi_rl','wgi_rq','wgi_va',\n",
    "    # treatment (do not impute treatment for FE logic) — we still examine missingness\n",
    "    'gain'\n",
    "]\n",
    "\n",
    "# add flags\n",
    "for col in candidate_covs:\n",
    "    flag_col = col + '_impflag'\n",
    "    df[flag_col] = df[col].isna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a2bd4",
   "metadata": {},
   "source": [
    "6) Build the baseline covariate lists to feed the DML nuisance learners\n",
    "\n",
    "We create (A) a baseline set (lagged core variables + WGI + lagged outcome), and (B) an extended set including additional candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b733f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline covariates for DML nuisance (lagged)\n",
    "baseline_covariates = [\n",
    "    'sovereign_spread_lag1',   # lagged outcome\n",
    "    'cpi_yoy_lag1',\n",
    "    'gdp_annual_growth_rate_lag1',\n",
    "    'ln_gdp_per_capita_lag1',\n",
    "    'debt_to_gdp_lag1',\n",
    "    'deficit_to_gdp_lag1',    # mediator caution — we use lag1\n",
    "    'current_account_balance_lag1',\n",
    "    # governance (level or lagged)\n",
    "    'wgi_cc', 'wgi_ge', 'wgi_pv', 'wgi_rl', 'wgi_rq', 'wgi_va',\n",
    "    # imputation flags for these\n",
    "    'sovereign_spread_lag1_impflag', 'cpi_yoy_lag1_impflag', 'gdp_annual_growth_rate_lag1_impflag',\n",
    "    'ln_gdp_per_capita_lag1_impflag', 'debt_to_gdp_lag1_impflag', 'deficit_to_gdp_lag1_impflag',\n",
    "    'current_account_balance_lag1_impflag'\n",
    "]\n",
    "\n",
    "# Extended covariates (if you have them) — add here\n",
    "extended_covariates = baseline_covariates + [\n",
    "    'sovereign_spread_lag2', 'cpi_yoy_lag2', 'gdp_annual_growth_rate_lag2', 'ln_gdp_per_capita_lag2',\n",
    "    'debt_to_gdp_lag2', 'deficit_to_gdp_lag2', 'current_account_balance_lag2',\n",
    "    # placeholder for global factors you might add:\n",
    "    # 'VIX_lag1', 'US10y_lag1', 'Brent_lag1',\n",
    "]\n",
    "\n",
    "# The treatment is 'gain' (do not include it in covariates)\n",
    "T_col = 'gain'\n",
    "Y_col = 'sovereign_spread'\n",
    "idcol = 'iso3c'\n",
    "timecol = 'year'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d88250",
   "metadata": {},
   "source": [
    "7) Fold-aware preprocessing function (impute on train only, scale on train only, optional FE partial-out)\n",
    "\n",
    "This is the core function to call inside your cross-fitting loop. It returns processed X_train, X_test, y_train, y_test, t_train, t_test and optionally the FE means and saved imputer/scaler objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0adce219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def fold_aware_preprocess(train_df, test_df, covariates,\n",
    "                          idcol='iso3c', ycol='sovereign_spread', tcol='gain',\n",
    "                          imputer=None, scaler=None, include_country_fe=False,\n",
    "                          save_prefix=None):\n",
    "    \"\"\"\n",
    "    - train_df/test_df are pandas DataFrames for the fold\n",
    "    - covariates: list of column names to use in X (these should include imputation flags)\n",
    "    - imputer: sklearn imputer instance (if None, uses KNNImputer(n_neighbors=5))\n",
    "    - scaler: sklearn scaler instance (if None, uses StandardScaler())\n",
    "    - include_country_fe: if True, compute country means on train and demean Y, T, and covariates (fe partial-out)\n",
    "    - save_prefix: optional path prefix to pickle imputer/scaler/fe_means\n",
    "    Returns: dict with processed arrays/dataframes and saved artifacts paths\n",
    "    \"\"\"\n",
    "    if imputer is None:\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    # 1) Impute covariates using training only\n",
    "    Xtrain_raw = train_df[covariates].copy()\n",
    "    Xtest_raw  = test_df[covariates].copy()\n",
    "    Xtrain_imp = pd.DataFrame(imputer.fit_transform(Xtrain_raw), columns=covariates, index=train_df.index)\n",
    "    Xtest_imp  = pd.DataFrame(imputer.transform(Xtest_raw), columns=covariates, index=test_df.index)\n",
    "\n",
    "    # save imputer\n",
    "    artifacts = {}\n",
    "    if save_prefix is not None:\n",
    "        p_imputer = f\"{save_prefix}_imputer.pkl\"\n",
    "        pickle.dump(imputer, open(p_imputer, 'wb'))\n",
    "        artifacts['imputer'] = p_imputer\n",
    "\n",
    "    # 2) optional: demean by country using training-based country means (for FE partial-out)\n",
    "    # We'll compute FE means for y, t, and covariates based on train only\n",
    "    if include_country_fe:\n",
    "        cols_for_means = [ycol, tcol] + covariates\n",
    "        # assemble train with imputed covariates for mean calc\n",
    "        train_for_means = train_df[[ycol, tcol]].join(Xtrain_imp)\n",
    "        fe_means = train_for_means.groupby(idcol).mean()\n",
    "        # global train means for countries not in train (rare in LOYO, but safe)\n",
    "        global_means = train_for_means.mean()\n",
    "\n",
    "        # join means into train/test\n",
    "        train_joined = train_for_means.join(fe_means, on=idcol, rsuffix='_mean')\n",
    "        test_joined  = test_df[[ycol, tcol]].join(Xtest_imp).join(fe_means, on=idcol, rsuffix='_mean')\n",
    "\n",
    "        # demean\n",
    "        for col in [ycol, tcol] + covariates:\n",
    "            mean_col = col + '_mean'\n",
    "            train_joined[col + '_d'] = train_joined[col] - train_joined[mean_col].fillna(global_means[col])\n",
    "            test_joined[col + '_d']  = test_joined[col]  - test_joined[mean_col].fillna(global_means[col])\n",
    "\n",
    "        # X matrices for ML are demeaned covariates\n",
    "        X_train = train_joined[[c + '_d' for c in covariates]].copy()\n",
    "        X_test  = test_joined[[c + '_d' for c in covariates]].copy()\n",
    "        y_train = train_joined[ycol + '_d'].copy()\n",
    "        y_test  = test_joined[ycol + '_d'].copy()\n",
    "        t_train = train_joined[tcol + '_d'].copy()\n",
    "        t_test  = test_joined[tcol + '_d'].copy()\n",
    "\n",
    "        # persist fe_means if requested\n",
    "        if save_prefix is not None:\n",
    "            p_femeans = f\"{save_prefix}_fe_means.pkl\"\n",
    "            pickle.dump(fe_means, open(p_femeans, 'wb'))\n",
    "            artifacts['fe_means'] = p_femeans\n",
    "\n",
    "    else:\n",
    "        # no FE partial-out: use imputed X directly\n",
    "        X_train = Xtrain_imp.copy()\n",
    "        X_test  = Xtest_imp.copy()\n",
    "        y_train = train_df[ycol].copy()\n",
    "        y_test  = test_df[ycol].copy()\n",
    "        t_train = train_df[tcol].copy()\n",
    "        t_test  = test_df[tcol].copy()\n",
    "\n",
    "    # 3) scale features using training-only scaler\n",
    "    X_train_cols = X_train.columns.tolist()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train_cols, index=X_train.index)\n",
    "    X_test_scaled  = pd.DataFrame(scaler.transform(X_test),  columns=X_train_cols, index=X_test.index)  # align columns\n",
    "\n",
    "    if save_prefix is not None:\n",
    "        p_scaler = f\"{save_prefix}_scaler.pkl\"\n",
    "        pickle.dump(scaler, open(p_scaler, 'wb'))\n",
    "        artifacts['scaler'] = p_scaler\n",
    "\n",
    "    out = {\n",
    "        'X_train': X_train_scaled, 'X_test': X_test_scaled,\n",
    "        'y_train': y_train, 'y_test': y_test,\n",
    "        't_train': t_train, 't_test': t_test,\n",
    "        'artifacts': artifacts\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286e7fd",
   "metadata": {},
   "source": [
    "8) Example: how to use the preprocessing function in a leave-one-year-out fold loop\n",
    "\n",
    "This snippet shows how to produce per-fold artifacts and store OOS p(X) R² for the demeaned T (helpful for FE diagnostic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c966b581",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artifacts/fold0_imputer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m test  \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[test_idx]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     16\u001b[0m save_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifacts/fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# change path as you see fit\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[43mfold_aware_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcovariates_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                             \u001b[49m\u001b[43midcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miso3c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mycol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mimputer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKNNImputer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStandardScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43minclude_country_fe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfe_include\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m prep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m], prep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m prep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m], prep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[20], line 31\u001b[0m, in \u001b[0;36mfold_aware_preprocess\u001b[0;34m(train_df, test_df, covariates, idcol, ycol, tcol, imputer, scaler, include_country_fe, save_prefix)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     p_imputer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_imputer.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(imputer, \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp_imputer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m     artifacts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m p_imputer\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 2) optional: demean by country using training-based country means (for FE partial-out)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# We'll compute FE means for y, t, and covariates based on train only\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'artifacts/fold0_imputer.pkl'"
     ]
    }
   ],
   "source": [
    "# Choose folds: leave-one-year-out\n",
    "years = sorted(df['year'].unique())\n",
    "folds = [(df.index[df['year'] != y].tolist(), df.index[df['year'] == y].tolist()) for y in years]\n",
    "\n",
    "# Which covariates to feed to preprocess (choose baseline or extended)\n",
    "covariates_to_use = baseline_covariates  # or extended_covariates\n",
    "\n",
    "p_oos_r2_list = []\n",
    "fe_include = True   # set True for FE-DML diagnostic; False for pooled spec\n",
    "all_theta_parts = []  # optionally store residuals\n",
    "\n",
    "for fnum, (train_idx, test_idx) in enumerate(folds):\n",
    "    train = df.loc[train_idx].copy()\n",
    "    test  = df.loc[test_idx].copy()\n",
    "\n",
    "    save_prefix = f\"artifacts/fold{fnum}\"  # change path as you see fit\n",
    "\n",
    "    prep = fold_aware_preprocess(train, test, covariates=covariates_to_use,\n",
    "                                 idcol='iso3c', ycol=Y_col, tcol=T_col,\n",
    "                                 imputer=KNNImputer(n_neighbors=5),\n",
    "                                 scaler=StandardScaler(),\n",
    "                                 include_country_fe=fe_include,\n",
    "                                 save_prefix=save_prefix)\n",
    "\n",
    "    X_train, X_test = prep['X_train'], prep['X_test']\n",
    "    y_train, y_test = prep['y_train'], prep['y_test']\n",
    "    t_train, t_test = prep['t_train'], prep['t_test']\n",
    "\n",
    "    # Fit p(X) on demeaned T (LassoCV)\n",
    "    p_model = LassoCV(cv=5, random_state=0).fit(X_train, t_train)\n",
    "    p_hat_test = p_model.predict(X_test)\n",
    "    # OOS R^2 for demeaned T\n",
    "    r2 = np.nan\n",
    "    if len(t_test)>0 and np.nanvar(t_test)>0:\n",
    "        r2 = r2_score(t_test, p_hat_test)\n",
    "    p_oos_r2_list.append(r2)\n",
    "\n",
    "    # Fit m(X) for demeaned Y (random forest)\n",
    "    m_model = RandomForestRegressor(n_estimators=200, random_state=0)\n",
    "    m_model.fit(X_train, y_train)\n",
    "    m_hat_test = m_model.predict(X_test)\n",
    "\n",
    "    # residuals for stacking later\n",
    "    u_hat = y_test.values - m_hat_test\n",
    "    v_hat = t_test.values - p_hat_test\n",
    "\n",
    "    # store in a dict or DataFrame for later stacking\n",
    "    tmp = pd.DataFrame({\n",
    "        'index': test.index,\n",
    "        'u_hat': u_hat,\n",
    "        'v_hat': v_hat\n",
    "    }).set_index('index')\n",
    "\n",
    "    all_theta_parts.append(tmp)\n",
    "\n",
    "# After loop: stack residuals and compute theta (example)\n",
    "stacked = pd.concat(all_theta_parts).sort_index()\n",
    "u_all = stacked['u_hat']\n",
    "v_all = stacked['v_hat']\n",
    "\n",
    "theta_hat = (v_all * u_all).sum() / (v_all**2).sum()\n",
    "print(\"Theta (DML-style point estimate from residuals):\", theta_hat)\n",
    "print(\"Median p(X) OOS R2 across folds:\", np.nanmedian(p_oos_r2_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572eed27",
   "metadata": {},
   "source": [
    "# Phase 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2fb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: imports and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# SETTINGS - EDIT THESE\n",
    "idcol = 'iso3c'\n",
    "timecol = 'year'\n",
    "Y_col = 'sovereign_spread'   # outcome\n",
    "T_col = 'gain'               # treatment (ND-GAIN)\n",
    "# Choose covariates prepared earlier (baseline from preprocessing)\n",
    "covariates_to_use = [\n",
    "    'sovereign_spread_lag1', 'cpi_yoy_lag1','gdp_annual_growth_rate_lag1',\n",
    "    'ln_gdp_per_capita_lag1','debt_to_gdp_lag1','deficit_to_gdp_lag1',\n",
    "    'current_account_balance_lag1',\n",
    "    'wgi_cc','wgi_ge','wgi_pv','wgi_rl','wgi_rq','wgi_va',\n",
    "    'sovereign_spread_lag1_impflag', 'cpi_yoy_lag1_impflag', 'gdp_annual_growth_rate_lag1_impflag',\n",
    "    'ln_gdp_per_capita_lag1_impflag','debt_to_gdp_lag1_impflag','deficit_to_gdp_lag1_impflag',\n",
    "    'current_account_balance_lag1_impflag'\n",
    "]\n",
    "\n",
    "# folds & artifacts\n",
    "fold_output_dir = \"artifacts/dml_pooled\"\n",
    "os.makedirs(fold_output_dir, exist_ok=True)\n",
    "\n",
    "# modeling choices\n",
    "n_trees = 300\n",
    "use_xgboost = False   # set True if xgboost is installed and you want to use it\n",
    "n_permutations = 200  # for permutation test (200 is reasonable; increase if you have time)\n",
    "random_seed = 2025\n",
    "np.random.seed(random_seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
