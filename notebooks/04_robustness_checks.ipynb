{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a3a415",
   "metadata": {},
   "source": [
    "# 04 Robustness Checks\n",
    "\n",
    "This notebook explores robustness checks for the Double ML results, including alternative machine learning models and subsampling by region or income group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba786973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Using cached statsmodels-0.14.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /opt/anaconda3/envs/mlenv/lib/python3.9/site-packages (from statsmodels) (2.0.2)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /opt/anaconda3/envs/mlenv/lib/python3.9/site-packages (from statsmodels) (1.13.1)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /opt/anaconda3/envs/mlenv/lib/python3.9/site-packages (from statsmodels) (2.2.3)\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\n",
      "  Using cached patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/mlenv/lib/python3.9/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/mlenv/lib/python3.9/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/mlenv/lib/python3.9/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/mlenv/lib/python3.9/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/mlenv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Using cached statsmodels-0.14.5-cp39-cp39-macosx_11_0_arm64.whl (9.8 MB)\n",
      "Using cached patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [statsmodels]\u001b[0m [statsmodels]\n",
      "\u001b[1A\u001b[2KSuccessfully installed patsy-1.0.1 statsmodels-0.14.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages (module name, pip name if different)\n",
    "required_packages = [\n",
    "    (\"statsmodels\", \"statsmodels\"),\n",
    "    (\"xgboost\", \"xgboost\"),\n",
    "    (\"sklearn\", \"scikit-learn\"),\n",
    "    (\"scipy\", \"scipy\"),\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "]\n",
    "\n",
    "for module_name, pip_name in required_packages:\n",
    "    install_if_missing(module_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b831ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script performs a comprehensive causal machine‑learning analysis of the\n",
    "relationship between lagged climate vulnerability and sovereign bond spreads.\n",
    "It includes:\n",
    "\n",
    "1.  Data preparation (loading, renaming columns, computing lags and dummy\n",
    "    variables).\n",
    "2.  Baseline Double Machine Learning (DML) with leave‑one‑year‑out cross‑fitting\n",
    "    for a binary “high‑spread” event (top 10 % of spreads).\n",
    "3.  Robustness checks: random permutation placebo, heterogeneity by income and\n",
    "    governance quality, regional splits.\n",
    "4.  First‑difference DML to remove time‑invariant fixed effects.\n",
    "5.  Instrumental‑variable (2SLS) regression using the second lag of\n",
    "    vulnerability as an instrument.\n",
    "6.  Linear quantile regressions at the 90th, 95th and 99th percentiles.\n",
    "7.  Quantile gradient boosting regressions at the 95th and 99th percentiles and\n",
    "    estimation of the marginal effect of vulnerability on these conditional\n",
    "    quantiles.\n",
    "\n",
    "The code is designed to be run in a notebook or standalone Python script.  It\n",
    "prints out key results for inclusion in an academic paper.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "\n",
    "def load_and_prepare_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the dataset, rename columns to avoid spaces, compute lags and dummy vars.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to the CSV file containing the baseline panel.\n",
    "\n",
    "    Returns:\n",
    "        A processed DataFrame ready for analysis.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path).sort_values([\"iso3c\", \"year\"]).reset_index(drop=True)\n",
    "    # Replace spaces in column names with underscores\n",
    "    df = df.rename(columns=lambda x: x.strip().replace(\" \", \"_\"))\n",
    "    # Compute lagged vulnerability (lag 1 and lag 2)\n",
    "    df[\"vulnerability_lag1\"] = df.groupby(\"iso3c\")[\"vulnerability\"].shift(1)\n",
    "    df[\"vulnerability_lag2\"] = df.groupby(\"iso3c\")[\"vulnerability\"].shift(2)\n",
    "    # Create high_spread event: top 10 % of spreads\n",
    "    thr = df[\"sovereign_spread\"].quantile(0.90)\n",
    "    df[\"high_spread\"] = (df[\"sovereign_spread\"] >= thr).astype(int)\n",
    "    # Create region dummy variables\n",
    "    region_dummies = pd.get_dummies(df[\"region\"], prefix=\"reg\")\n",
    "    df = pd.concat([df, region_dummies], axis=1)\n",
    "    # Compute an overall WGI score per row and classify governance groups\n",
    "    wgi_cols = [c for c in df.columns if c.startswith(\"wgi\")]\n",
    "    df[\"wgi_score\"] = df[wgi_cols].mean(axis=1)\n",
    "    avg_wgi_country = df.groupby(\"iso3c\")[\"wgi_score\"].mean()\n",
    "    median_wgi = avg_wgi_country.median()\n",
    "    df[\"governance_group\"] = df[\"iso3c\"].apply(\n",
    "        lambda iso: \"High_Gov\" if avg_wgi_country[iso] > median_wgi else \"Low_Gov\"\n",
    "    )\n",
    "    # Compute an income classification based on median GDP per capita across countries\n",
    "    avg_gdp_country = df.groupby(\"iso3c\")[\"gdp_per_capita\"].mean()\n",
    "    median_gdp = avg_gdp_country.median()\n",
    "    df[\"income_group\"] = df[\"iso3c\"].apply(\n",
    "        lambda iso: \"High\" if avg_gdp_country[iso] > median_gdp else \"Low\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_loyo_dml(Y, T, X, years, groups, n_neighbors=5, n_estimators=100, random_state=7):\n",
    "    \"\"\"Run leave‑one‑year‑out DML for a binary outcome.\n",
    "\n",
    "    Args:\n",
    "        Y: array of outcome values (0/1 for tail risk event).\n",
    "        T: array of treatment values (lagged vulnerability).\n",
    "        X: 2D array of covariates.\n",
    "        years: array of year identifiers for cross‑fitting splits.\n",
    "        groups: array of cluster identifiers (countries).\n",
    "        n_neighbors: number of neighbors for KNN imputation.\n",
    "        n_estimators: number of trees in XGBoost models.\n",
    "        random_state: random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        theta, se, pval – the treatment effect, standard error and p‑value.\n",
    "    \"\"\"\n",
    "    N = len(Y)\n",
    "    Yres = np.zeros(N)\n",
    "    Tres = np.zeros(N)\n",
    "    unique_years = np.unique(years)\n",
    "    for yr in unique_years:\n",
    "        test_idx = np.where(years == yr)[0]\n",
    "        train_idx = np.where(years != yr)[0]\n",
    "        # Impute and standardize covariates within fold\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        scaler = StandardScaler()\n",
    "        X_train = imputer.fit_transform(X[train_idx])\n",
    "        X_test = imputer.transform(X[test_idx])\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        T_train = T[train_idx]\n",
    "        Y_train = Y[train_idx]\n",
    "        # Treatment model (regressor)\n",
    "        mdl_T = xgb.XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        mdl_T.fit(X_train, T_train)\n",
    "        p_hat = mdl_T.predict(X_test)\n",
    "        # Outcome model (classifier)\n",
    "        mdl_Y = xgb.XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        mdl_Y.fit(X_train, Y_train)\n",
    "        m_hat = mdl_Y.predict_proba(X_test)[:, 1]\n",
    "        # Residuals\n",
    "        Yres[test_idx] = Y[test_idx] - m_hat\n",
    "        Tres[test_idx] = T[test_idx] - p_hat\n",
    "    # Second stage regression with cluster‑robust SEs\n",
    "    res = sm.OLS(Yres, Tres).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups})\n",
    "    theta = res.params[0]\n",
    "    se = res.bse[0]\n",
    "    pval = res.pvalues[0]\n",
    "    return theta, se, pval\n",
    "\n",
    "\n",
    "def dml_tail_risk_analysis(df: pd.DataFrame, q=0.10):\n",
    "    \"\"\"Run baseline tail‑risk DML and heterogeneity checks.\n",
    "\n",
    "    Args:\n",
    "        df: processed DataFrame.\n",
    "        q: quantile to define the high‑spread event (default 0.10 for top 10 %).\n",
    "    Returns:\n",
    "        A dictionary of results.\n",
    "    \"\"\"\n",
    "    # Identify high‑spread event based on quantile q\n",
    "    thr = df[\"sovereign_spread\"].quantile(1 - q)\n",
    "    df[\"high_spread\"] = (df[\"sovereign_spread\"] >= thr).astype(int)\n",
    "    # Define feature set\n",
    "    wgi_cols = [c for c in df.columns if c.startswith(\"wgi\")]\n",
    "    macro_cols = [\n",
    "        \"cpi_yoy\",\n",
    "        \"gdp_annual_growth_rate\",\n",
    "        \"gdp_per_capita\",\n",
    "        \"gross_gdp\",\n",
    "        \"debt_to_gdp\",\n",
    "        \"deficit_to_gdp\",\n",
    "        \"current_account_balance\",\n",
    "        \"population\",\n",
    "        \"mineral_rent\",\n",
    "        \"gain\",\n",
    "    ]\n",
    "    cat_cols = [c for c in df.columns if c.startswith(\"reg_\")]\n",
    "    X_cols = wgi_cols + macro_cols + cat_cols\n",
    "    # Baseline mask\n",
    "    mask_all = df[\"vulnerability_lag1\"].notna() & df[\"high_spread\"].notna()\n",
    "    Y = df.loc[mask_all, \"high_spread\"].values\n",
    "    T = df.loc[mask_all, \"vulnerability_lag1\"].values\n",
    "    years = df.loc[mask_all, \"year\"].values\n",
    "    groups = df.loc[mask_all, \"iso3c\"].values\n",
    "    X = df.loc[mask_all, X_cols].values\n",
    "    # Baseline DML\n",
    "    theta, se, pval = run_loyo_dml(Y, T, X, years, groups, n_neighbors=5, n_estimators=200)\n",
    "    results = {\"baseline_theta\": theta, \"baseline_se\": se, \"baseline_pval\": pval}\n",
    "    # Placebo: random permutation of treatment\n",
    "    np.random.seed(42)\n",
    "    T_perm = np.random.permutation(T)\n",
    "    theta_perm, se_perm, pval_perm = run_loyo_dml(Y, T_perm, X, years, groups, n_neighbors=5, n_estimators=200)\n",
    "    results.update({\n",
    "        \"perm_theta\": theta_perm,\n",
    "        \"perm_se\": se_perm,\n",
    "        \"perm_pval\": pval_perm,\n",
    "    })\n",
    "    # Heterogeneity by income\n",
    "    for group_name in [\"Low\", \"High\"]:\n",
    "        mask = (df[\"income_group\"] == group_name) & mask_all\n",
    "        Yg, Tg, Xg = df.loc[mask, \"high_spread\"].values, df.loc[mask, \"vulnerability_lag1\"].values, df.loc[mask, X_cols].values\n",
    "        yearsg = df.loc[mask, \"year\"].values\n",
    "        groupsg = df.loc[mask, \"iso3c\"].values\n",
    "        theta_g, se_g, p_g = run_loyo_dml(Yg, Tg, Xg, yearsg, groupsg, n_neighbors=5, n_estimators=200)\n",
    "        results[f\"income_{group_name.lower()}_theta\"] = theta_g\n",
    "        results[f\"income_{group_name.lower()}_se\"] = se_g\n",
    "        results[f\"income_{group_name.lower()}_pval\"] = p_g\n",
    "    # Income difference significance\n",
    "    theta_low = results[\"income_low_theta\"]\n",
    "    se_low = results[\"income_low_se\"]\n",
    "    theta_high = results[\"income_high_theta\"]\n",
    "    se_high = results[\"income_high_se\"]\n",
    "    diff = theta_low - theta_high\n",
    "    se_diff = np.sqrt(se_low ** 2 + se_high ** 2)\n",
    "    z_score = diff / se_diff\n",
    "    p_diff = 2 * (1 - norm.cdf(abs(z_score)))\n",
    "    results.update({\n",
    "        \"income_diff\": diff,\n",
    "        \"income_diff_se\": se_diff,\n",
    "        \"income_diff_pval\": p_diff,\n",
    "    })\n",
    "    # Heterogeneity by governance quality\n",
    "    for group_name in [\"Low_Gov\", \"High_Gov\"]:\n",
    "        mask = (df[\"governance_group\"] == group_name) & mask_all\n",
    "        Yg, Tg, Xg = df.loc[mask, \"high_spread\"].values, df.loc[mask, \"vulnerability_lag1\"].values, df.loc[mask, X_cols].values\n",
    "        yearsg = df.loc[mask, \"year\"].values\n",
    "        groupsg = df.loc[mask, \"iso3c\"].values\n",
    "        theta_g, se_g, p_g = run_loyo_dml(Yg, Tg, Xg, yearsg, groupsg, n_neighbors=5, n_estimators=200)\n",
    "        results[f\"gov_{group_name.lower()}_theta\"] = theta_g\n",
    "        results[f\"gov_{group_name.lower()}_se\"] = se_g\n",
    "        results[f\"gov_{group_name.lower()}_pval\"] = p_g\n",
    "    # Governance difference significance\n",
    "    theta_low_g = results[\"gov_low_gov_theta\"]\n",
    "    se_low_g = results[\"gov_low_gov_se\"]\n",
    "    theta_high_g = results[\"gov_high_gov_theta\"]\n",
    "    se_high_g = results[\"gov_high_gov_se\"]\n",
    "    diff_g = theta_low_g - theta_high_g\n",
    "    se_diff_g = np.sqrt(se_low_g ** 2 + se_high_g ** 2)\n",
    "    z_score_g = diff_g / se_diff_g\n",
    "    p_diff_g = 2 * (1 - norm.cdf(abs(z_score_g)))\n",
    "    results.update({\n",
    "        \"gov_diff\": diff_g,\n",
    "        \"gov_diff_se\": se_diff_g,\n",
    "        \"gov_diff_pval\": p_diff_g,\n",
    "    })\n",
    "    # Regional splits (only regions with >100 observations)\n",
    "    regions = df[\"region\"].unique()\n",
    "    region_results = {}\n",
    "    for reg in regions:\n",
    "        mask_reg = (df[\"region\"] == reg) & mask_all\n",
    "        n_reg = mask_reg.sum()\n",
    "        if n_reg > 100:\n",
    "            Yg, Tg, Xg = df.loc[mask_reg, \"high_spread\"].values, df.loc[mask_reg, \"vulnerability_lag1\"].values, df.loc[mask_reg, X_cols].values\n",
    "            yearsg = df.loc[mask_reg, \"year\"].values\n",
    "            groupsg = df.loc[mask_reg, \"iso3c\"].values\n",
    "            theta_g, se_g, p_g = run_loyo_dml(Yg, Tg, Xg, yearsg, groupsg, n_neighbors=5, n_estimators=200)\n",
    "            region_results[reg] = (theta_g, se_g, p_g, n_reg)\n",
    "    results[\"region_results\"] = region_results\n",
    "    return results\n",
    "\n",
    "\n",
    "def first_difference_dml(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Run first‑difference DML for the change in high_spread vs change in vulnerability.\n",
    "\n",
    "    This removes time‑invariant country fixed effects following Clarke & Polselli.\n",
    "    \"\"\"\n",
    "    # Compute differenced variables per country\n",
    "    # List of covariates to difference\n",
    "    feature_cols = [\n",
    "        c\n",
    "        for c in df.columns\n",
    "        if c.startswith(\"wgi\")\n",
    "        or c\n",
    "        in [\n",
    "            \"cpi_yoy\",\n",
    "            \"gdp_annual_growth_rate\",\n",
    "            \"gdp_per_capita\",\n",
    "            \"gross_gdp\",\n",
    "            \"debt_to_gdp\",\n",
    "            \"deficit_to_gdp\",\n",
    "            \"current_account_balance\",\n",
    "            \"population\",\n",
    "            \"mineral_rent\",\n",
    "            \"gain\",\n",
    "        ]\n",
    "    ]\n",
    "    diff_dfs = []\n",
    "    for iso, g in df.groupby(\"iso3c\"):\n",
    "        g = g.sort_values(\"year\")\n",
    "        # Compute first differences of treatment, outcome and controls\n",
    "        diff_data = {f\"diff_{col}\": g[col].diff() for col in feature_cols}\n",
    "        diff_data[\"diffY\"] = g[\"high_spread\"].diff()\n",
    "        diff_data[\"diffT\"] = g[\"vulnerability_lag1\"].diff()\n",
    "        diff_data[\"iso3c\"] = iso\n",
    "        diff_data[\"year\"] = g[\"year\"].values\n",
    "        diff_df = pd.DataFrame(diff_data)\n",
    "        diff_dfs.append(diff_df)\n",
    "    fd_df = pd.concat(diff_dfs).dropna()\n",
    "    # Prepare arrays\n",
    "    Y = fd_df[\"diffY\"].values\n",
    "    T = fd_df[\"diffT\"].values\n",
    "    years = fd_df[\"year\"].values\n",
    "    groups = fd_df[\"iso3c\"].values\n",
    "    X = fd_df[[c for c in fd_df.columns if c.startswith(\"diff_\") and c not in [\"diffY\", \"diffT\"]]].values\n",
    "    # Run LOYO DML on differenced data using regressor for both Y and T\n",
    "    N = len(Y)\n",
    "    Yres = np.zeros(N)\n",
    "    Tres = np.zeros(N)\n",
    "    for yr in np.unique(years):\n",
    "        test_idx = np.where(years == yr)[0]\n",
    "        train_idx = np.where(years != yr)[0]\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        scaler = StandardScaler()\n",
    "        X_train = imputer.fit_transform(X[train_idx])\n",
    "        X_test = imputer.transform(X[test_idx])\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        T_train = T[train_idx]\n",
    "        Y_train = Y[train_idx]\n",
    "        mdl_T = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=7,\n",
    "        )\n",
    "        mdl_T.fit(X_train, T_train)\n",
    "        p_hat = mdl_T.predict(X_test)\n",
    "        mdl_Y = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=7,\n",
    "        )\n",
    "        mdl_Y.fit(X_train, Y_train)\n",
    "        m_hat = mdl_Y.predict(X_test)\n",
    "        Yres[test_idx] = Y[test_idx] - m_hat\n",
    "        Tres[test_idx] = T[test_idx] - p_hat\n",
    "    res = sm.OLS(Yres, Tres).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups})\n",
    "    return res.params[0], res.bse[0], res.pvalues[0]\n",
    "\n",
    "\n",
    "def instrumental_variable_analysis(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Run a two‑stage least squares (2SLS) regression for the high_spread indicator.\n",
    "\n",
    "    The second lag of vulnerability is used as an instrument for the first lag.\n",
    "    \"\"\"\n",
    "    # Select rows with all needed variables non‑null\n",
    "    wgi_cols = [c for c in df.columns if c.startswith(\"wgi\")]\n",
    "    macro_cols = [\n",
    "        \"cpi_yoy\",\n",
    "        \"gdp_annual_growth_rate\",\n",
    "        \"gdp_per_capita\",\n",
    "        \"gross_gdp\",\n",
    "        \"debt_to_gdp\",\n",
    "        \"deficit_to_gdp\",\n",
    "        \"current_account_balance\",\n",
    "        \"population\",\n",
    "        \"mineral_rent\",\n",
    "        \"gain\",\n",
    "    ]\n",
    "    cat_cols = [c for c in df.columns if c.startswith(\"reg_\")]\n",
    "    X_cols = wgi_cols + macro_cols + cat_cols\n",
    "    mask = df[\"high_spread\"].notna() & df[\"vulnerability_lag1\"].notna() & df[\"vulnerability_lag2\"].notna()\n",
    "    sub = df.loc[mask].reset_index(drop=True)\n",
    "    # Impute missing covariates with KNN\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X_imp = pd.DataFrame(imputer.fit_transform(sub[X_cols]), columns=X_cols)\n",
    "    # Stage 1: regress vulnerability_lag1 on vulnerability_lag2 and controls\n",
    "    X1 = sm.add_constant(pd.concat([sub[\"vulnerability_lag2\"], X_imp], axis=1))\n",
    "    mod1 = sm.OLS(sub[\"vulnerability_lag1\"], X1).fit()\n",
    "    T_hat = mod1.fittedvalues.rename(\"T_hat\")\n",
    "    # Stage 2: regress high_spread on T_hat and controls\n",
    "    X2 = sm.add_constant(pd.concat([T_hat, X_imp], axis=1))\n",
    "    mod2 = sm.OLS(sub[\"high_spread\"], X2).fit(cov_type=\"cluster\", cov_kwds={\"groups\": sub[\"iso3c\"]})\n",
    "    coef = mod2.params[\"T_hat\"]\n",
    "    se = mod2.bse[\"T_hat\"]\n",
    "    pval = mod2.pvalues[\"T_hat\"]\n",
    "    return coef, se, pval\n",
    "\n",
    "\n",
    "def quantile_regressions(df: pd.DataFrame):\n",
    "    \"\"\"Run linear quantile regressions on sovereign spreads.\n",
    "    Returns a dict with coefficients and p‑values for vulnerability at q=0.90,0.95,0.99.\n",
    "    \"\"\"\n",
    "    import statsmodels.formula.api as smf\n",
    "    df_qr = df.copy()\n",
    "    # Remove rows with any missing in outcome, treatment or controls\n",
    "    wgi_cols = [c for c in df_qr.columns if c.startswith(\"wgi\")]\n",
    "    macro_cols = [\n",
    "        \"cpi_yoy\",\n",
    "        \"gdp_annual_growth_rate\",\n",
    "        \"gdp_per_capita\",\n",
    "        \"gross_gdp\",\n",
    "        \"debt_to_gdp\",\n",
    "        \"deficit_to_gdp\",\n",
    "        \"current_account_balance\",\n",
    "        \"population\",\n",
    "        \"mineral_rent\",\n",
    "        \"gain\",\n",
    "    ]\n",
    "    model_formula = \"sovereign_spread ~ vulnerability_lag1 + \" + \" + \".join(wgi_cols + macro_cols)\n",
    "    # Drop rows with NaNs in formula variables\n",
    "    mask = df_qr[[\"sovereign_spread\", \"vulnerability_lag1\"] + wgi_cols + macro_cols].notnull().all(axis=1)\n",
    "    df_clean = df_qr.loc[mask]\n",
    "    results = {}\n",
    "    for q in [0.90, 0.95, 0.99]:\n",
    "        try:\n",
    "            mod = smf.quantreg(model_formula, df_clean).fit(q=q)\n",
    "            coef = mod.params[\"vulnerability_lag1\"]\n",
    "            pval = mod.pvalues[\"vulnerability_lag1\"]\n",
    "            results[q] = (coef, pval)\n",
    "        except Exception:\n",
    "            results[q] = (np.nan, np.nan)\n",
    "    return results\n",
    "\n",
    "\n",
    "def quantile_gradient_boosting(df: pd.DataFrame):\n",
    "    \"\"\"Estimate the effect of vulnerability on the 95th and 99th percentiles of spreads\n",
    "    using quantile gradient boosting.  Returns a dict with the estimated mean\n",
    "    change in predicted quantiles (per 1‑SD increase in vulnerability) and its\n",
    "    t‑statistic/p‑value.\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    wgi_cols = [c for c in df.columns if c.startswith(\"wgi\")]\n",
    "    macro_cols = [\n",
    "        \"cpi_yoy\",\n",
    "        \"gdp_annual_growth_rate\",\n",
    "        \"gdp_per_capita\",\n",
    "        \"gross_gdp\",\n",
    "        \"debt_to_gdp\",\n",
    "        \"deficit_to_gdp\",\n",
    "        \"current_account_balance\",\n",
    "        \"population\",\n",
    "        \"mineral_rent\",\n",
    "        \"gain\",\n",
    "    ]\n",
    "    cat_cols = [c for c in df.columns if c.startswith(\"reg_\")]\n",
    "    X_cols = [\"vulnerability_lag1\"] + wgi_cols + macro_cols + cat_cols\n",
    "    mask = df[X_cols + [\"sovereign_spread\"]].notnull().all(axis=1)\n",
    "    sub = df.loc[mask].reset_index(drop=True)\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X = pd.DataFrame(imputer.fit_transform(sub[X_cols]), columns=X_cols)\n",
    "    Y = sub[\"sovereign_spread\"].values\n",
    "    # Compute standard deviation of vulnerability for scaling\n",
    "    std_vul = X[\"vulnerability_lag1\"].std()\n",
    "    results = {}\n",
    "    for alpha in [0.95, 0.99]:\n",
    "        gbr = GradientBoostingRegressor(\n",
    "            loss=\"quantile\",\n",
    "            alpha=alpha,\n",
    "            n_estimators=300,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42,\n",
    "        )\n",
    "        gbr.fit(X, Y)\n",
    "        pred_base = gbr.predict(X)\n",
    "        X_high = X.copy()\n",
    "        X_high[\"vulnerability_lag1\"] = X_high[\"vulnerability_lag1\"] + std_vul\n",
    "        pred_high = gbr.predict(X_high)\n",
    "        diff = pred_high - pred_base\n",
    "        mean_diff = diff.mean()\n",
    "        # Standard error of the mean difference\n",
    "        se_diff = diff.std(ddof=1) / np.sqrt(len(diff))\n",
    "        t_stat = mean_diff / se_diff\n",
    "        pval = 2 * (1 - t.cdf(np.abs(t_stat), df=len(diff) - 1))\n",
    "        results[alpha] = {\n",
    "            \"mean_diff\": mean_diff,\n",
    "            \"se_diff\": se_diff,\n",
    "            \"t_stat\": t_stat,\n",
    "            \"pval\": pval,\n",
    "        }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # Path to CSV (adjust as necessary)\n",
    "    csv_path = \"baseline_with_gain_population_mineral_regions.csv\"\n",
    "    df = load_and_prepare_data(csv_path)\n",
    "    # 1. Baseline DML and heterogeneity\n",
    "    dml_results = dml_tail_risk_analysis(df, q=0.10)\n",
    "    print(\"\\n=== Baseline DML and Heterogeneity Results ===\")\n",
    "    print(\n",
    "        f\"Baseline effect: θ = {dml_results['baseline_theta']:.4f}, SE = {dml_results['baseline_se']:.4f}, p = {dml_results['baseline_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Placebo (permuted treatment): θ = {dml_results['perm_theta']:.4f}, SE = {dml_results['perm_se']:.4f}, p = {dml_results['perm_pval']:.4f}\"\n",
    "    )\n",
    "    print(\"\\nIncome heterogeneity:\")\n",
    "    print(\n",
    "        f\"  Low income: θ = {dml_results['income_low_theta']:.4f}, SE = {dml_results['income_low_se']:.4f}, p = {dml_results['income_low_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High income: θ = {dml_results['income_high_theta']:.4f}, SE = {dml_results['income_high_se']:.4f}, p = {dml_results['income_high_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Difference (low – high): diff = {dml_results['income_diff']:.4f}, SE = {dml_results['income_diff_se']:.4f}, p = {dml_results['income_diff_pval']:.4f}\"\n",
    "    )\n",
    "    print(\"\\nGovernance heterogeneity:\")\n",
    "    print(\n",
    "        f\"  Low governance: θ = {dml_results['gov_low_gov_theta']:.4f}, SE = {dml_results['gov_low_gov_se']:.4f}, p = {dml_results['gov_low_gov_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High governance: θ = {dml_results['gov_high_gov_theta']:.4f}, SE = {dml_results['gov_high_gov_se']:.4f}, p = {dml_results['gov_high_gov_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Difference (low – high): diff = {dml_results['gov_diff']:.4f}, SE = {dml_results['gov_diff_se']:.4f}, p = {dml_results['gov_diff_pval']:.4f}\"\n",
    "    )\n",
    "    print(\"\\nRegion heterogeneity (regions with >100 observations):\")\n",
    "    for reg, (theta, se, p, n_reg) in dml_results[\"region_results\"].items():\n",
    "        print(f\"  {reg}: θ = {theta:.3f}, SE = {se:.3f}, p = {p:.3f}, n = {n_reg}\")\n",
    "    # 2. First‑difference DML\n",
    "    fd_theta, fd_se, fd_p = first_difference_dml(df)\n",
    "    print(\"\\n=== First‑difference DML ===\")\n",
    "    print(f\"θ = {fd_theta:.4f}, SE = {fd_se:.4f}, p = {fd_p:.4f}\")\n",
    "    # 3. Instrumental variables analysis\n",
    "    iv_coef, iv_se, iv_p = instrumental_variable_analysis(df)\n",
    "    print(\"\\n=== Instrumental‑Variable (2SLS) Analysis ===\")\n",
    "    print(f\"Coef = {iv_coef:.4f}, SE = {iv_se:.4f}, p = {iv_p:.4f}\")\n",
    "    # 4. Linear quantile regression results\n",
    "    qr_results = quantile_regressions(df)\n",
    "    print(\"\\n=== Linear Quantile Regression ===\")\n",
    "    for q, (coef, pval) in qr_results.items():\n",
    "        print(f\"  Quantile {int(q*100)}%: coef = {coef:.6f}, p = {pval:.4e}\")\n",
    "    # 5. Quantile gradient boosting results\n",
    "    qgb_results = quantile_gradient_boosting(df)\n",
    "    print(\"\\n=== Quantile Gradient Boosting (95th and 99th percentiles) ===\")\n",
    "    for alpha, stats in qgb_results.items():\n",
    "        print(\n",
    "            f\"  α = {alpha:.2f}: mean ΔQ = {stats['mean_diff']:.4f}, SE = {stats['se_diff']:.4f}, t = {stats['t_stat']:.2f}, p = {stats['pval']:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
