{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a3a415",
   "metadata": {},
   "source": [
    "# 02 analysis\n",
    "This notebook explores  our baseline and robustness for the Double ML results, including alternative machine learning models and subsampling by region or income group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba786973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages (module name, pip name if different)\n",
    "required_packages = [\n",
    "    (\"statsmodels\", \"statsmodels\"),\n",
    "    (\"xgboost\", \"xgboost\"),\n",
    "    (\"sklearn\", \"scikit-learn\"),\n",
    "    (\"scipy\", \"scipy\"),\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "]\n",
    "\n",
    "for module_name, pip_name in required_packages:\n",
    "    install_if_missing(module_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b831ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script performs a comprehensive causal machine‑learning analysis of the\n",
    "relationship between lagged climate vulnerability and sovereign bond spreads.\n",
    "It includes:\n",
    "\n",
    "1.  Data preparation (loading, renaming columns, computing lags and dummy\n",
    "    variables).\n",
    "2.  Baseline Double Machine Learning (DML) with leave‑one‑year‑out cross‑fitting\n",
    "    for a binary “high‑spread” event (top 10 % of spreads).\n",
    "3.  Robustness checks: random permutation placebo, heterogeneity by income and\n",
    "    governance quality, regional splits.\n",
    "4.  First‑difference DML to remove time‑invariant fixed effects.\n",
    "5.  Instrumental‑variable (2SLS) regression using the second lag of\n",
    "    vulnerability as an instrument.\n",
    "6.  Linear quantile regressions at the 90th, 95th and 99th percentiles.\n",
    "7.  Quantile gradient boosting regressions at the 95th and 99th percentiles and\n",
    "    estimation of the marginal effect of vulnerability on these conditional\n",
    "    quantiles.\n",
    "\n",
    "The code is designed to be run in a notebook or standalone Python script.  It\n",
    "prints out key results for inclusion in an academic paper.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "\n",
    "def load_and_prepare_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the dataset, rename columns to avoid spaces, compute lags and dummy vars.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to the CSV file containing the baseline panel.\n",
    "\n",
    "    Returns:\n",
    "        A processed DataFrame ready for analysis.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path).sort_values([\"iso3c\", \"year\"]).reset_index(drop=True)\n",
    "    # Replace spaces in column names with underscores\n",
    "    df = df.rename(columns=lambda x: x.strip().replace(\" \", \"_\"))\n",
    "    # Compute lagged vulnerability (lag 1 and lag 2)\n",
    "    df[\"vulnerability_lag1\"] = df.groupby(\"iso3c\")[\"vulnerability\"].shift(1)\n",
    "    df[\"vulnerability_lag2\"] = df.groupby(\"iso3c\")[\"vulnerability\"].shift(2)\n",
    "    # Create high_spread event: top 10 % of spreads\n",
    "    thr = df[\"sovereign_spread\"].quantile(0.90)\n",
    "    df[\"high_spread\"] = (df[\"sovereign_spread\"] >= thr).astype(int)\n",
    "    # Create region dummy variables\n",
    "    region_dummies = pd.get_dummies(df[\"region\"], prefix=\"reg\")\n",
    "    df = pd.concat([df, region_dummies], axis=1)\n",
    "    # Compute an overall WGI score per row and classify governance groups\n",
    "    wgi_cols = [c for c in df.columns if c.startswith(\"wgi\")]\n",
    "    df[\"wgi_score\"] = df[wgi_cols].mean(axis=1)\n",
    "    avg_wgi_country = df.groupby(\"iso3c\")[\"wgi_score\"].mean()\n",
    "    median_wgi = avg_wgi_country.median()\n",
    "    df[\"governance_group\"] = df[\"iso3c\"].apply(\n",
    "        lambda iso: \"High_Gov\" if avg_wgi_country[iso] > median_wgi else \"Low_Gov\"\n",
    "    )\n",
    "    # Compute an income classification based on median GDP per capita across countries\n",
    "    avg_gdp_country = df.groupby(\"iso3c\")[\"gdp_per_capita\"].mean()\n",
    "    median_gdp = avg_gdp_country.median()\n",
    "    df[\"income_group\"] = df[\"iso3c\"].apply(\n",
    "        lambda iso: \"High\" if avg_gdp_country[iso] > median_gdp else \"Low\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_loyo_dml(Y, T, X, years, groups, n_neighbors=5, n_estimators=100, random_state=7):\n",
    "    \"\"\"Run leave‑one‑year‑out DML for a binary outcome.\n",
    "\n",
    "    Args:\n",
    "        Y: array of outcome values (0/1 for tail risk event).\n",
    "        T: array of treatment values (lagged vulnerability).\n",
    "        X: 2D array of covariates.\n",
    "        years: array of year identifiers for cross‑fitting splits.\n",
    "        groups: array of cluster identifiers (countries).\n",
    "        n_neighbors: number of neighbors for KNN imputation.\n",
    "        n_estimators: number of trees in XGBoost models.\n",
    "        random_state: random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        theta, se, pval – the treatment effect, standard error and p‑value.\n",
    "    \"\"\"\n",
    "    N = len(Y)\n",
    "    Yres = np.zeros(N)\n",
    "    Tres = np.zeros(N)\n",
    "    unique_years = np.unique(years)\n",
    "    for yr in unique_years:\n",
    "        test_idx = np.where(years == yr)[0]\n",
    "        train_idx = np.where(years != yr)[0]\n",
    "        # Impute and standardize covariates within fold\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        scaler = StandardScaler()\n",
    "        X_train = imputer.fit_transform(X[train_idx])\n",
    "        X_test = imputer.transform(X[test_idx])\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        T_train = T[train_idx]\n",
    "        Y_train = Y[train_idx]\n",
    "        # Treatment model (regressor)\n",
    "        mdl_T = xgb.XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        mdl_T.fit(X_train, T_train)\n",
    "        p_hat = mdl_T.predict(X_test)\n",
    "        # Outcome model (classifier)\n",
    "        mdl_Y = xgb.XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        mdl_Y.fit(X_train, Y_train)\n",
    "        m_hat = mdl_Y.predict_proba(X_test)[:, 1]\n",
    "        # Residuals\n",
    "        Yres[test_idx] = Y[test_idx] - m_hat\n",
    "        Tres[test_idx] = T[test_idx] - p_hat\n",
    "    # Second stage regression with cluster‑robust SEs\n",
    "    res = sm.OLS(Yres, Tres).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups})\n",
    "    theta = res.params[0]\n",
    "    se = res.bse[0]\n",
    "    pval = res.pvalues[0]\n",
    "    return theta, se, pval\n",
    "\n",
    "\n",
    "def dml_tail_risk_analysis(df: pd.DataFrame, q=0.10):\n",
    "    \"\"\"Run baseline tail‑risk DML and heterogeneity checks.\n",
    "\n",
    "    Args:\n",
    "        df: processed DataFrame.\n",
    "        q: quantile to define the high‑spread event (default 0.10 for top 10 %).\n",
    "    Returns:\n",
    "        A dictionary of results.\n",
    "    \"\"\"\n",
    "    # Identify high‑spread event based on quantile q\n",
    "    thr = df[\"sovereign_spread\"].quantile(1 - q)\n",
    "    df[\"high_spread\"] = (df[\"sovereign_spread\"] >= thr).astype(int)\n",
    "    # Define feature set\n",
    "    wgi_cols = [c for c in df.columns if c.startswith(\"wgi\")]\n",
    "    macro_cols = [\n",
    "        \"cpi_yoy\",\n",
    "        \"gdp_annual_growth_rate\",\n",
    "        \"gdp_per_capita\",\n",
    "        \"gross_gdp\",\n",
    "        \"debt_to_gdp\",\n",
    "        \"deficit_to_gdp\",\n",
    "        \"current_account_balance\",\n",
    "        \"population\",\n",
    "        \"mineral_rent\",\n",
    "        \"gain\",\n",
    "    ]\n",
    "    cat_cols = [c for c in df.columns if c.startswith(\"reg_\")]\n",
    "    X_cols = wgi_cols + macro_cols + cat_cols\n",
    "    # Baseline mask\n",
    "    mask_all = df[\"vulnerability_lag1\"].notna() & df[\"high_spread\"].notna()\n",
    "    Y = df.loc[mask_all, \"high_spread\"].values\n",
    "    T = df.loc[mask_all, \"vulnerability_lag1\"].values\n",
    "    years = df.loc[mask_all, \"year\"].values\n",
    "    groups = df.loc[mask_all, \"iso3c\"].values\n",
    "    X = df.loc[mask_all, X_cols].values\n",
    "    # Baseline DML\n",
    "    theta, se, pval = run_loyo_dml(Y, T, X, years, groups, n_neighbors=5, n_estimators=200)\n",
    "    results = {\"baseline_theta\": theta, \"baseline_se\": se, \"baseline_pval\": pval}\n",
    "    # Placebo: random permutation of treatment\n",
    "    np.random.seed(42)\n",
    "    T_perm = np.random.permutation(T)\n",
    "    theta_perm, se_perm, pval_perm = run_loyo_dml(Y, T_perm, X, years, groups, n_neighbors=5, n_estimators=200)\n",
    "    results.update({\n",
    "        \"perm_theta\": theta_perm,\n",
    "        \"perm_se\": se_perm,\n",
    "        \"perm_pval\": pval_perm,\n",
    "    })\n",
    "    # Heterogeneity by income\n",
    "    for group_name in [\"Low\", \"High\"]:\n",
    "        mask = (df[\"income_group\"] == group_name) & mask_all\n",
    "        Yg, Tg, Xg = df.loc[mask, \"high_spread\"].values, df.loc[mask, \"vulnerability_lag1\"].values, df.loc[mask, X_cols].values\n",
    "        yearsg = df.loc[mask, \"year\"].values\n",
    "        groupsg = df.loc[mask, \"iso3c\"].values\n",
    "        theta_g, se_g, p_g = run_loyo_dml(Yg, Tg, Xg, yearsg, groupsg, n_neighbors=5, n_estimators=200)\n",
    "        results[f\"income_{group_name.lower()}_theta\"] = theta_g\n",
    "        results[f\"income_{group_name.lower()}_se\"] = se_g\n",
    "        results[f\"income_{group_name.lower()}_pval\"] = p_g\n",
    "    # Income difference significance\n",
    "    theta_low = results[\"income_low_theta\"]\n",
    "    se_low = results[\"income_low_se\"]\n",
    "    theta_high = results[\"income_high_theta\"]\n",
    "    se_high = results[\"income_high_se\"]\n",
    "    diff = theta_low - theta_high\n",
    "    se_diff = np.sqrt(se_low ** 2 + se_high ** 2)\n",
    "    z_score = diff / se_diff\n",
    "    p_diff = 2 * (1 - norm.cdf(abs(z_score)))\n",
    "    results.update({\n",
    "        \"income_diff\": diff,\n",
    "        \"income_diff_se\": se_diff,\n",
    "        \"income_diff_pval\": p_diff,\n",
    "    })\n",
    "    # Heterogeneity by governance quality\n",
    "    for group_name in [\"Low_Gov\", \"High_Gov\"]:\n",
    "        mask = (df[\"governance_group\"] == group_name) & mask_all\n",
    "        Yg, Tg, Xg = df.loc[mask, \"high_spread\"].values, df.loc[mask, \"vulnerability_lag1\"].values, df.loc[mask, X_cols].values\n",
    "        yearsg = df.loc[mask, \"year\"].values\n",
    "        groupsg = df.loc[mask, \"iso3c\"].values\n",
    "        theta_g, se_g, p_g = run_loyo_dml(Yg, Tg, Xg, yearsg, groupsg, n_neighbors=5, n_estimators=200)\n",
    "        results[f\"gov_{group_name.lower()}_theta\"] = theta_g\n",
    "        results[f\"gov_{group_name.lower()}_se\"] = se_g\n",
    "        results[f\"gov_{group_name.lower()}_pval\"] = p_g\n",
    "    # Governance difference significance\n",
    "    theta_low_g = results[\"gov_low_gov_theta\"]\n",
    "    se_low_g = results[\"gov_low_gov_se\"]\n",
    "    theta_high_g = results[\"gov_high_gov_theta\"]\n",
    "    se_high_g = results[\"gov_high_gov_se\"]\n",
    "    diff_g = theta_low_g - theta_high_g\n",
    "    se_diff_g = np.sqrt(se_low_g ** 2 + se_high_g ** 2)\n",
    "    z_score_g = diff_g / se_diff_g\n",
    "    p_diff_g = 2 * (1 - norm.cdf(abs(z_score_g)))\n",
    "    results.update({\n",
    "        \"gov_diff\": diff_g,\n",
    "        \"gov_diff_se\": se_diff_g,\n",
    "        \"gov_diff_pval\": p_diff_g,\n",
    "    })\n",
    "    # Regional splits (only regions with >100 observations)\n",
    "    regions = df[\"region\"].unique()\n",
    "    region_results = {}\n",
    "    for reg in regions:\n",
    "        mask_reg = (df[\"region\"] == reg) & mask_all\n",
    "        n_reg = mask_reg.sum()\n",
    "        if n_reg > 100:\n",
    "            Yg, Tg, Xg = df.loc[mask_reg, \"high_spread\"].values, df.loc[mask_reg, \"vulnerability_lag1\"].values, df.loc[mask_reg, X_cols].values\n",
    "            yearsg = df.loc[mask_reg, \"year\"].values\n",
    "            groupsg = df.loc[mask_reg, \"iso3c\"].values\n",
    "            theta_g, se_g, p_g = run_loyo_dml(Yg, Tg, Xg, yearsg, groupsg, n_neighbors=5, n_estimators=200)\n",
    "            region_results[reg] = (theta_g, se_g, p_g, n_reg)\n",
    "    results[\"region_results\"] = region_results\n",
    "    return results\n",
    "\n",
    "\n",
    "def first_difference_dml(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Run first‑difference DML for the change in high_spread vs change in vulnerability.\n",
    "\n",
    "    This removes time‑invariant country fixed effects following Clarke & Polselli.\n",
    "    \"\"\"\n",
    "    # Compute differenced variables per country\n",
    "    # List of covariates to difference\n",
    "    feature_cols = [\n",
    "        c\n",
    "        for c in df.columns\n",
    "        if c.startswith(\"wgi\")\n",
    "        or c\n",
    "        in [\n",
    "            \"cpi_yoy\",\n",
    "            \"gdp_annual_growth_rate\",\n",
    "            \"gdp_per_capita\",\n",
    "            \"gross_gdp\",\n",
    "            \"debt_to_gdp\",\n",
    "            \"deficit_to_gdp\",\n",
    "            \"current_account_balance\",\n",
    "            \"population\",\n",
    "            \"mineral_rent\",\n",
    "            \"gain\",\n",
    "        ]\n",
    "    ]\n",
    "    diff_dfs = []\n",
    "    for iso, g in df.groupby(\"iso3c\"):\n",
    "        g = g.sort_values(\"year\")\n",
    "        # Compute first differences of treatment, outcome and controls\n",
    "        diff_data = {f\"diff_{col}\": g[col].diff() for col in feature_cols}\n",
    "        diff_data[\"diffY\"] = g[\"high_spread\"].diff()\n",
    "        diff_data[\"diffT\"] = g[\"vulnerability_lag1\"].diff()\n",
    "        diff_data[\"iso3c\"] = iso\n",
    "        diff_data[\"year\"] = g[\"year\"].values\n",
    "        diff_df = pd.DataFrame(diff_data)\n",
    "        diff_dfs.append(diff_df)\n",
    "    fd_df = pd.concat(diff_dfs).dropna()\n",
    "    # Prepare arrays\n",
    "    Y = fd_df[\"diffY\"].values\n",
    "    T = fd_df[\"diffT\"].values\n",
    "    years = fd_df[\"year\"].values\n",
    "    groups = fd_df[\"iso3c\"].values\n",
    "    X = fd_df[[c for c in fd_df.columns if c.startswith(\"diff_\") and c not in [\"diffY\", \"diffT\"]]].values\n",
    "    # Run LOYO DML on differenced data using regressor for both Y and T\n",
    "    N = len(Y)\n",
    "    Yres = np.zeros(N)\n",
    "    Tres = np.zeros(N)\n",
    "    for yr in np.unique(years):\n",
    "        test_idx = np.where(years == yr)[0]\n",
    "        train_idx = np.where(years != yr)[0]\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        scaler = StandardScaler()\n",
    "        X_train = imputer.fit_transform(X[train_idx])\n",
    "        X_test = imputer.transform(X[test_idx])\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        T_train = T[train_idx]\n",
    "        Y_train = Y[train_idx]\n",
    "        mdl_T = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=7,\n",
    "        )\n",
    "        mdl_T.fit(X_train, T_train)\n",
    "        p_hat = mdl_T.predict(X_test)\n",
    "        mdl_Y = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=7,\n",
    "        )\n",
    "        mdl_Y.fit(X_train, Y_train)\n",
    "        m_hat = mdl_Y.predict(X_test)\n",
    "        Yres[test_idx] = Y[test_idx] - m_hat\n",
    "        Tres[test_idx] = T[test_idx] - p_hat\n",
    "    res = sm.OLS(Yres, Tres).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups})\n",
    "    return res.params[0], res.bse[0], res.pvalues[0]\n",
    "\n",
    "\n",
    "def instrumental_variable_analysis(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Run a two‑stage least squares (2SLS) regression for the high_spread indicator.\n",
    "\n",
    "    The second lag of vulnerability is used as an instrument for the first lag.\n",
    "    \"\"\"\n",
    "    # Select rows with all needed variables non‑null\n",
    "    wgi_cols = [c for c in df.columns if c.startswith(\"wgi\")]\n",
    "    macro_cols = [\n",
    "        \"cpi_yoy\",\n",
    "        \"gdp_annual_growth_rate\",\n",
    "        \"gdp_per_capita\",\n",
    "        \"gross_gdp\",\n",
    "        \"debt_to_gdp\",\n",
    "        \"deficit_to_gdp\",\n",
    "        \"current_account_balance\",\n",
    "        \"population\",\n",
    "        \"mineral_rent\",\n",
    "        \"gain\",\n",
    "    ]\n",
    "    cat_cols = [c for c in df.columns if c.startswith(\"reg_\")]\n",
    "    X_cols = wgi_cols + macro_cols + cat_cols\n",
    "    mask = df[\"high_spread\"].notna() & df[\"vulnerability_lag1\"].notna() & df[\"vulnerability_lag2\"].notna()\n",
    "    sub = df.loc[mask].reset_index(drop=True)\n",
    "    # Impute missing covariates with KNN\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X_imp = pd.DataFrame(imputer.fit_transform(sub[X_cols]), columns=X_cols)\n",
    "    # Stage 1: regress vulnerability_lag1 on vulnerability_lag2 and controls\n",
    "    X1 = sm.add_constant(pd.concat([sub[\"vulnerability_lag2\"], X_imp], axis=1))\n",
    "    mod1 = sm.OLS(sub[\"vulnerability_lag1\"], X1).fit()\n",
    "    T_hat = mod1.fittedvalues.rename(\"T_hat\")\n",
    "    # Stage 2: regress high_spread on T_hat and controls\n",
    "    X2 = sm.add_constant(pd.concat([T_hat, X_imp], axis=1))\n",
    "    mod2 = sm.OLS(sub[\"high_spread\"], X2).fit(cov_type=\"cluster\", cov_kwds={\"groups\": sub[\"iso3c\"]})\n",
    "    coef = mod2.params[\"T_hat\"]\n",
    "    se = mod2.bse[\"T_hat\"]\n",
    "    pval = mod2.pvalues[\"T_hat\"]\n",
    "    return coef, se, pval\n",
    "\n",
    "\n",
    "def quantile_regressions(df: pd.DataFrame):\n",
    "    \"\"\"Run linear quantile regressions on sovereign spreads.\n",
    "    Returns a dict with coefficients and p‑values for vulnerability at q=0.90,0.95,0.99.\n",
    "    \"\"\"\n",
    "    import statsmodels.formula.api as smf\n",
    "    df_qr = df.copy()\n",
    "    # Remove rows with any missing in outcome, treatment or controls\n",
    "    wgi_cols = [c for c in df_qr.columns if c.startswith(\"wgi\")]\n",
    "    macro_cols = [\n",
    "        \"cpi_yoy\",\n",
    "        \"gdp_annual_growth_rate\",\n",
    "        \"gdp_per_capita\",\n",
    "        \"gross_gdp\",\n",
    "        \"debt_to_gdp\",\n",
    "        \"deficit_to_gdp\",\n",
    "        \"current_account_balance\",\n",
    "        \"population\",\n",
    "        \"mineral_rent\",\n",
    "        \"gain\",\n",
    "    ]\n",
    "    model_formula = \"sovereign_spread ~ vulnerability_lag1 + \" + \" + \".join(wgi_cols + macro_cols)\n",
    "    # Drop rows with NaNs in formula variables\n",
    "    mask = df_qr[[\"sovereign_spread\", \"vulnerability_lag1\"] + wgi_cols + macro_cols].notnull().all(axis=1)\n",
    "    df_clean = df_qr.loc[mask]\n",
    "    results = {}\n",
    "    for q in [0.90, 0.95, 0.99]:\n",
    "        try:\n",
    "            mod = smf.quantreg(model_formula, df_clean).fit(q=q)\n",
    "            coef = mod.params[\"vulnerability_lag1\"]\n",
    "            pval = mod.pvalues[\"vulnerability_lag1\"]\n",
    "            results[q] = (coef, pval)\n",
    "        except Exception:\n",
    "            results[q] = (np.nan, np.nan)\n",
    "    return results\n",
    "\n",
    "\n",
    "def quantile_gradient_boosting(df: pd.DataFrame):\n",
    "    \"\"\"Estimate the effect of vulnerability on the 95th and 99th percentiles of spreads\n",
    "    using quantile gradient boosting.  Returns a dict with the estimated mean\n",
    "    change in predicted quantiles (per 1‑SD increase in vulnerability) and its\n",
    "    t‑statistic/p‑value.\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    wgi_cols = [c for c in df.columns if c.startswith(\"wgi\")]\n",
    "    macro_cols = [\n",
    "        \"cpi_yoy\",\n",
    "        \"gdp_annual_growth_rate\",\n",
    "        \"gdp_per_capita\",\n",
    "        \"gross_gdp\",\n",
    "        \"debt_to_gdp\",\n",
    "        \"deficit_to_gdp\",\n",
    "        \"current_account_balance\",\n",
    "        \"population\",\n",
    "        \"mineral_rent\",\n",
    "        \"gain\",\n",
    "    ]\n",
    "    cat_cols = [c for c in df.columns if c.startswith(\"reg_\")]\n",
    "    X_cols = [\"vulnerability_lag1\"] + wgi_cols + macro_cols + cat_cols\n",
    "    mask = df[X_cols + [\"sovereign_spread\"]].notnull().all(axis=1)\n",
    "    sub = df.loc[mask].reset_index(drop=True)\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X = pd.DataFrame(imputer.fit_transform(sub[X_cols]), columns=X_cols)\n",
    "    Y = sub[\"sovereign_spread\"].values\n",
    "    # Compute standard deviation of vulnerability for scaling\n",
    "    std_vul = X[\"vulnerability_lag1\"].std()\n",
    "    results = {}\n",
    "    for alpha in [0.95, 0.99]:\n",
    "        gbr = GradientBoostingRegressor(\n",
    "            loss=\"quantile\",\n",
    "            alpha=alpha,\n",
    "            n_estimators=300,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42,\n",
    "        )\n",
    "        gbr.fit(X, Y)\n",
    "        pred_base = gbr.predict(X)\n",
    "        X_high = X.copy()\n",
    "        X_high[\"vulnerability_lag1\"] = X_high[\"vulnerability_lag1\"] + std_vul\n",
    "        pred_high = gbr.predict(X_high)\n",
    "        diff = pred_high - pred_base\n",
    "        mean_diff = diff.mean()\n",
    "        # Standard error of the mean difference\n",
    "        se_diff = diff.std(ddof=1) / np.sqrt(len(diff))\n",
    "        t_stat = mean_diff / se_diff\n",
    "        pval = 2 * (1 - t.cdf(np.abs(t_stat), df=len(diff) - 1))\n",
    "        results[alpha] = {\n",
    "            \"mean_diff\": mean_diff,\n",
    "            \"se_diff\": se_diff,\n",
    "            \"t_stat\": t_stat,\n",
    "            \"pval\": pval,\n",
    "        }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7041504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns(df: pd.DataFrame):\n",
    "    \"\"\"Build X_cols consistently wherever needed.\"\"\"\n",
    "    wgi_cols = [c for c in df.columns if c.startswith(\"wgi\")]\n",
    "    macro_cols = [\n",
    "        \"cpi_yoy\", \"gdp_annual_growth_rate\", \"gdp_per_capita\", \"gross_gdp\",\n",
    "        \"debt_to_gdp\", \"deficit_to_gdp\", \"current_account_balance\",\n",
    "        \"population\", \"mineral_rent\", \"gain\",\n",
    "    ]\n",
    "    cat_cols = [c for c in df.columns if c.startswith(\"reg_\")]\n",
    "    X_cols = wgi_cols + macro_cols + cat_cols\n",
    "    return X_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2572c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def placebo_temporal_lead(df: pd.DataFrame, q: float = 0.10):\n",
    "    \"\"\"Use future vulnerability (lead) as treatment; should be ~0.\"\"\"\n",
    "    df_ = df.copy()\n",
    "    # ensure high_spread is present (recompute to be safe and consistent with q)\n",
    "    thr = df_[\"sovereign_spread\"].quantile(1 - q)\n",
    "    df_[\"high_spread\"] = (df_[\"sovereign_spread\"] >= thr).astype(int)\n",
    "\n",
    "    # treatment = lead of vulnerability\n",
    "    df_[\"vulnerability_lead1\"] = df_.groupby(\"iso3c\")[\"vulnerability\"].shift(-1)\n",
    "\n",
    "    X_cols = get_feature_columns(df_)\n",
    "    mask = df_[\"vulnerability_lead1\"].notna() & df_[\"high_spread\"].notna()\n",
    "    Y = df_.loc[mask, \"high_spread\"].values\n",
    "    T = df_.loc[mask, \"vulnerability_lead1\"].values\n",
    "    X = df_.loc[mask, X_cols].values\n",
    "    years = df_.loc[mask, \"year\"].values\n",
    "    groups = df_.loc[mask, \"iso3c\"].values\n",
    "\n",
    "    return run_loyo_dml(Y, T, X, years, groups, n_neighbors=5, n_estimators=200)\n",
    "\n",
    "\n",
    "def placebo_within_year_shuffle(df: pd.DataFrame, q: float = 0.10, random_state: int = 42):\n",
    "    \"\"\"Shuffle vulnerability across countries within each year; should be ~0.\"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    df_ = df.copy()\n",
    "    thr = df_[\"sovereign_spread\"].quantile(1 - q)\n",
    "    df_[\"high_spread\"] = (df_[\"sovereign_spread\"] >= thr).astype(int)\n",
    "\n",
    "    # start from lagged vulnerability (your baseline treatment)\n",
    "    df_[\"T_shuffled\"] = df_[\"vulnerability_lag1\"]\n",
    "    for yr, g in df_.groupby(\"year\").groups.items():\n",
    "        idx = df_.index.isin(df_.loc[g].index)\n",
    "        mask_year = idx & df_[\"T_shuffled\"].notna()\n",
    "        vals = df_.loc[mask_year, \"T_shuffled\"].values\n",
    "        df_.loc[mask_year, \"T_shuffled\"] = rng.permutation(vals)\n",
    "\n",
    "    X_cols = get_feature_columns(df_)\n",
    "    mask = df_[\"T_shuffled\"].notna() & df_[\"high_spread\"].notna()\n",
    "    Y = df_.loc[mask, \"high_spread\"].values\n",
    "    T = df_.loc[mask, \"T_shuffled\"].values\n",
    "    X = df_.loc[mask, X_cols].values\n",
    "    years = df_.loc[mask, \"year\"].values\n",
    "    groups = df_.loc[mask, \"iso3c\"].values\n",
    "\n",
    "    return run_loyo_dml(Y, T, X, years, groups, n_neighbors=5, n_estimators=200)\n",
    "\n",
    "\n",
    "def placebo_within_country_time_scramble(df: pd.DataFrame, q: float = 0.10, random_state: int = 123):\n",
    "    \"\"\"\n",
    "    NEW placebo: randomize the time order of vulnerability *within each country*.\n",
    "    Preserves each country's distribution and country FE but destroys timing.\n",
    "    Should be ~0 if identification comes from correct temporal variation.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    df_ = df.copy()\n",
    "    thr = df_[\"sovereign_spread\"].quantile(1 - q)\n",
    "    df_[\"high_spread\"] = (df_[\"sovereign_spread\"] >= thr).astype(int)\n",
    "\n",
    "    df_[\"T_scrambled\"] = np.nan\n",
    "    for iso, gidx in df_.groupby(\"iso3c\").groups.items():\n",
    "        idx = df_.index.isin(df_.loc[gidx].index)\n",
    "        mask_iso = idx & df_[\"vulnerability_lag1\"].notna()\n",
    "        vals = df_.loc[mask_iso, \"vulnerability_lag1\"].values\n",
    "        if len(vals) > 1:\n",
    "            df_.loc[mask_iso, \"T_scrambled\"] = rng.permutation(vals)\n",
    "        else:\n",
    "            # if only one observation, keep it (won't drive results)\n",
    "            df_.loc[mask_iso, \"T_scrambled\"] = vals\n",
    "\n",
    "    X_cols = get_feature_columns(df_)\n",
    "    mask = df_[\"T_scrambled\"].notna() & df_[\"high_spread\"].notna()\n",
    "    Y = df_.loc[mask, \"high_spread\"].values\n",
    "    T = df_.loc[mask, \"T_scrambled\"].values\n",
    "    X = df_.loc[mask, X_cols].values\n",
    "    years = df_.loc[mask, \"year\"].values\n",
    "    groups = df_.loc[mask, \"iso3c\"].values\n",
    "\n",
    "    return run_loyo_dml(Y, T, X, years, groups, n_neighbors=5, n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cdd320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline DML and Heterogeneity Results ===\n",
      "Baseline effect: θ = 0.6897, SE = 0.2175, p = 0.0015\n",
      "Placebo (permuted treatment): θ = 0.1198, SE = 0.0474, p = 0.0116\n",
      "\n",
      "Income heterogeneity:\n",
      "  Low income: θ = 2.1155, SE = 0.5382, p = 0.0001\n",
      "  High income: θ = 0.2015, SE = 0.1818, p = 0.2677\n",
      "  Difference (low – high): diff = 1.9140, SE = 0.5681, p = 0.0008\n",
      "\n",
      "Governance heterogeneity:\n",
      "  Low governance: θ = 1.5541, SE = 0.6204, p = 0.0122\n",
      "  High governance: θ = 0.2141, SE = 0.1590, p = 0.1781\n",
      "  Difference (low – high): diff = 1.3400, SE = 0.6404, p = 0.0364\n",
      "\n",
      "Region heterogeneity (regions with >100 observations):\n",
      "  Asia: θ = 0.135, SE = 0.662, p = 0.838, n = 493\n",
      "  South America: θ = 3.498, SE = 5.873, p = 0.551, n = 145\n",
      "  Europe: θ = 0.095, SE = 0.274, p = 0.730, n = 928\n",
      "  Africa: θ = 2.431, SE = 3.123, p = 0.436, n = 261\n",
      "\n",
      "=== First‑difference DML ===\n",
      "θ = 2.7099, SE = 1.6312, p = 0.0967\n",
      "\n",
      "=== Instrumental‑Variable (2SLS) Analysis ===\n",
      "Coef = 0.5291, SE = 0.3339, p = 0.1131\n",
      "\n",
      "=== Linear Quantile Regression ===\n",
      "  Quantile 90%: coef = 0.002582, p = 0.0000e+00\n",
      "  Quantile 95%: coef = 0.003429, p = 2.7668e-322\n",
      "  Quantile 99%: coef = 0.001929, p = 2.2145e-276\n",
      "\n",
      "=== Quantile Gradient Boosting (95th and 99th percentiles) ===\n",
      "  α = 0.95: mean ΔQ = 0.0623, SE = 0.0049, t = 12.63, p = 0.0000\n",
      "  α = 0.99: mean ΔQ = 0.0251, SE = 0.0048, t = 5.22, p = 0.0000\n",
      "\n",
      "=== Additional Placebo Tests ===\n",
      "Temporal placebo (future vulnerability): θ = 0.4156, SE = 0.2515, p = 0.0984\n",
      "Within-year shuffle placebo:            θ = -0.1876, SE = 0.0634, p = 0.0031\n",
      "Within-country time-scramble placebo:   θ = 0.5150, SE = 0.1753, p = 0.0033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # Path to CSV (adjust as necessary)\n",
    "    csv_path = \"/Users/leosgambato/Documents/GitHub/Capstone/data/processed/baseline_with_gain_population_mineral_regions.csv\"\n",
    "    df = load_and_prepare_data(csv_path)\n",
    "    # 1. Baseline DML and heterogeneity\n",
    "    dml_results = dml_tail_risk_analysis(df, q=0.10)\n",
    "    print(\"\\n=== Baseline DML and Heterogeneity Results ===\")\n",
    "    print(\n",
    "        f\"Baseline effect: θ = {dml_results['baseline_theta']:.4f}, SE = {dml_results['baseline_se']:.4f}, p = {dml_results['baseline_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Placebo (permuted treatment): θ = {dml_results['perm_theta']:.4f}, SE = {dml_results['perm_se']:.4f}, p = {dml_results['perm_pval']:.4f}\"\n",
    "    )\n",
    "    print(\"\\nIncome heterogeneity:\")\n",
    "    print(\n",
    "        f\"  Low income: θ = {dml_results['income_low_theta']:.4f}, SE = {dml_results['income_low_se']:.4f}, p = {dml_results['income_low_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High income: θ = {dml_results['income_high_theta']:.4f}, SE = {dml_results['income_high_se']:.4f}, p = {dml_results['income_high_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Difference (low – high): diff = {dml_results['income_diff']:.4f}, SE = {dml_results['income_diff_se']:.4f}, p = {dml_results['income_diff_pval']:.4f}\"\n",
    "    )\n",
    "    print(\"\\nGovernance heterogeneity:\")\n",
    "    print(\n",
    "        f\"  Low governance: θ = {dml_results['gov_low_gov_theta']:.4f}, SE = {dml_results['gov_low_gov_se']:.4f}, p = {dml_results['gov_low_gov_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High governance: θ = {dml_results['gov_high_gov_theta']:.4f}, SE = {dml_results['gov_high_gov_se']:.4f}, p = {dml_results['gov_high_gov_pval']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Difference (low – high): diff = {dml_results['gov_diff']:.4f}, SE = {dml_results['gov_diff_se']:.4f}, p = {dml_results['gov_diff_pval']:.4f}\"\n",
    "    )\n",
    "    print(\"\\nRegion heterogeneity (regions with >100 observations):\")\n",
    "    for reg, (theta, se, p, n_reg) in dml_results[\"region_results\"].items():\n",
    "        print(f\"  {reg}: θ = {theta:.3f}, SE = {se:.3f}, p = {p:.3f}, n = {n_reg}\")\n",
    "    # 2. First‑difference DML\n",
    "    fd_theta, fd_se, fd_p = first_difference_dml(df)\n",
    "    print(\"\\n=== First‑difference DML ===\")\n",
    "    print(f\"θ = {fd_theta:.4f}, SE = {fd_se:.4f}, p = {fd_p:.4f}\")\n",
    "    # 3. Instrumental variables analysis\n",
    "    iv_coef, iv_se, iv_p = instrumental_variable_analysis(df)\n",
    "    print(\"\\n=== Instrumental‑Variable (2SLS) Analysis ===\")\n",
    "    print(f\"Coef = {iv_coef:.4f}, SE = {iv_se:.4f}, p = {iv_p:.4f}\")\n",
    "    # 4. Linear quantile regression results\n",
    "    qr_results = quantile_regressions(df)\n",
    "    print(\"\\n=== Linear Quantile Regression ===\")\n",
    "    for q, (coef, pval) in qr_results.items():\n",
    "        print(f\"  Quantile {int(q*100)}%: coef = {coef:.6f}, p = {pval:.4e}\")\n",
    "    # 5. Quantile gradient boosting results\n",
    "    qgb_results = quantile_gradient_boosting(df)\n",
    "    print(\"\\n=== Quantile Gradient Boosting (95th and 99th percentiles) ===\")\n",
    "    for alpha, stats in qgb_results.items():\n",
    "        print(\n",
    "            f\"  α = {alpha:.2f}: mean ΔQ = {stats['mean_diff']:.4f}, SE = {stats['se_diff']:.4f}, t = {stats['t_stat']:.2f}, p = {stats['pval']:.4f}\"\n",
    "        )\n",
    "\n",
    "# --- Additional placebo tests ---\n",
    "    print(\"\\n=== Additional Placebo Tests ===\")\n",
    "\n",
    "    # 1) Temporal lead placebo\n",
    "    theta_lead, se_lead, p_lead = placebo_temporal_lead(df, q=0.10)\n",
    "    print(f\"Temporal placebo (future vulnerability): θ = {theta_lead:.4f}, SE = {se_lead:.4f}, p = {p_lead:.4f}\")\n",
    "\n",
    "    # 2) Random assignment within year (country shuffle within each year)\n",
    "    theta_shuf, se_shuf, p_shuf = placebo_within_year_shuffle(df, q=0.10, random_state=42)\n",
    "    print(f\"Within-year shuffle placebo:            θ = {theta_shuf:.4f}, SE = {se_shuf:.4f}, p = {p_shuf:.4f}\")\n",
    "\n",
    "    # 3) NEW: Within-country time scramble (destroys timing, keeps country distribution)\n",
    "    theta_time, se_time, p_time = placebo_within_country_time_scramble(df, q=0.10, random_state=123)\n",
    "    print(f\"Within-country time-scramble placebo:   θ = {theta_time:.4f}, SE = {se_time:.4f}, p = {p_time:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed114111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (lagged vulnerability) effect: (1.0244335653836223, 0.36373056969813206, 0.0048555687894364236, np.float64(7.79085396057985))\n",
      "Temporal placebo (future vulnerability) effect: (0.19483356555222453, 0.2511390750842593, 0.43786738705820505, np.float64(1.4819946994050153))\n",
      "Random country assignment effect: (-0.027654262914394684, 0.05596763455656098, 0.621227297162621, np.float64(-0.21031166005659724))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m res_rand \u001b[38;5;241m=\u001b[39m run_dml_binary(T_shuffled, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh_spread\u001b[39m\u001b[38;5;124m'\u001b[39m], X_base, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso3c\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom country assignment effect:\u001b[39m\u001b[38;5;124m'\u001b[39m, res_rand)\n\u001b[0;32m--> 112\u001b[0m \u001b[43mPY\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PY' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/Users/leosgambato/Documents/GitHub/Capstone/data/processed/baseline_with_gain_population_mineral_regions.csv\").sort_values(['iso3c','year']).reset_index(drop=True)\n",
    "# compute vulnerability lag and lead\n",
    "# compute vulnerability lag and lead\n",
    "\n",
    "df['vulnerability_lag1'] = df.groupby('iso3c')['vulnerability'].shift(1)\n",
    "df['vulnerability_lead1'] = df.groupby('iso3c')['vulnerability'].shift(-1)\n",
    "\n",
    "# outcome: high spread indicator top 10% global\n",
    "top_q = 0.10\n",
    "threshold = df['sovereign_spread'].quantile(1-top_q) # 90th percentile threshold (makes top 10%)\n",
    "df['high_spread'] = (df['sovereign_spread'] >= threshold).astype(int)\n",
    "\n",
    "# build covariate matrix (same as earlier): macro, WGI, region dummies, lags of macro\n",
    "macro_vars = ['cpi_yoy','gdp_annual_growth_rate','gdp_per_capita','gross gdp','debt_to_gdp','deficit_to_gdp','current_account_balance','population','mineral_rent','gain']\n",
    "# compute macro lag1 for some variables\n",
    "for col in ['cpi_yoy','gdp_annual_growth_rate','debt_to_gdp','deficit_to_gdp','gain','current_account_balance']:\n",
    "    df[f'{col}_lag1'] = df.groupby('iso3c')[col].shift(1)\n",
    "\n",
    "# updated macro var list with lags\n",
    "macro_vars_full = macro_vars + ['cpi_yoy_lag1','gdp_annual_growth_rate_lag1','debt_to_gdp_lag1','deficit_to_gdp_lag1','gain_lag1','current_account_balance_lag1']\n",
    "\n",
    "wgi_vars = ['wgi_cc','wgi_ge','wgi_pv','wgi_rl','wgi_rq','wgi_va']\n",
    "region_dummies = pd.get_dummies(df['region'], prefix='reg', dummy_na=True)\n",
    "\n",
    "X_base = pd.concat([df[macro_vars_full + wgi_vars], region_dummies], axis=1)\n",
    "\n",
    "\n",
    "def run_dml_binary(T, Y, X, years, iso, n_neighbors=5, trees=200, depth=4, lr=0.05, seed=7):\n",
    "    # Remove missing\n",
    "    mask = (~T.isna()) & (~Y.isna())\n",
    "    T = T[mask].reset_index(drop=True)\n",
    "    Y = Y[mask].reset_index(drop=True)\n",
    "    X = X.loc[mask].reset_index(drop=True)\n",
    "    years = years[mask].reset_index(drop=True)\n",
    "    iso = iso[mask].reset_index(drop=True)\n",
    "\n",
    "    # Unique years\n",
    "    uyears = np.sort(years.unique())\n",
    "\n",
    "    Yres = np.zeros(len(Y))\n",
    "    Tres = np.zeros(len(T))\n",
    "\n",
    "    for yr in uyears:\n",
    "        test_idx = np.where(years == yr)[0]\n",
    "        train_idx = np.where(years != yr)[0]\n",
    "        # impute and standardize\n",
    "        imp = KNNImputer(n_neighbors=n_neighbors)\n",
    "        X_tr = pd.DataFrame(imp.fit_transform(X.iloc[train_idx]), columns=X.columns)\n",
    "        X_te = pd.DataFrame(imp.transform(X.iloc[test_idx]), columns=X.columns)\n",
    "        scaler = StandardScaler()\n",
    "        X_tr = pd.DataFrame(scaler.fit_transform(X_tr), columns=X.columns)\n",
    "        X_te = pd.DataFrame(scaler.transform(X_te), columns=X.columns)\n",
    "        # treatment model (regressor)\n",
    "        mdl_T = xgb.XGBRegressor(n_estimators=trees, max_depth=depth, learning_rate=lr,\n",
    "                                 subsample=0.8, colsample_bytree=0.8, random_state=seed)\n",
    "        mdl_T.fit(X_tr, T.iloc[train_idx])\n",
    "        p_hat = mdl_T.predict(X_te)\n",
    "        # outcome model (classifier) - logistic/regression for binary but we treat as classification; we will use probability of 1\n",
    "        mdl_Y = xgb.XGBClassifier(n_estimators=trees, max_depth=depth, learning_rate=lr,\n",
    "                                 subsample=0.8, colsample_bytree=0.8, random_state=seed)\n",
    "        mdl_Y.fit(X_tr, Y.iloc[train_idx])\n",
    "        m_hat = mdl_Y.predict_proba(X_te)[:,1]\n",
    "        # residuals\n",
    "        Yres[test_idx] = Y.iloc[test_idx] - m_hat\n",
    "        Tres[test_idx] = T.iloc[test_idx] - p_hat\n",
    "\n",
    "    # second stage OLS without intercept\n",
    "    model = sm.OLS(Yres, Tres)\n",
    "    res = model.fit(cov_type='cluster', cov_kwds={'groups': iso})\n",
    "    theta = float(res.params[0])\n",
    "    se = float(res.bse[0])\n",
    "    p = float(res.pvalues[0])\n",
    "    effect_pp = theta * T.std() * 100  # percentage point change per 1 SD increase\n",
    "    return theta, se, p, effect_pp\n",
    "\n",
    "# run baseline for reference (with lagged vulnerability treatment)\n",
    "res_baseline = run_dml_binary(df['vulnerability_lag1'], df['high_spread'], X_base, df['year'], df['iso3c'])\n",
    "print('Baseline (lagged vulnerability) effect:', res_baseline)\n",
    "\n",
    "# Temporal placebo: future vulnerability (lead) as treatment\n",
    "res_lead = run_dml_binary(df['vulnerability_lead1'], df['high_spread'], X_base, df['year'], df['iso3c'])\n",
    "print('Temporal placebo (future vulnerability) effect:', res_lead)\n",
    "\n",
    "# Random country assignment: shuffle vulnerability within each year across countries\n",
    "# We'll shuffle vulnerability_lag1 within year\n",
    "np.random.seed(42)\n",
    "T_shuffled = df['vulnerability_lag1'].copy()\n",
    "for yr in df['year'].unique():\n",
    "    idx = df['year'] == yr\n",
    "    T_year = T_shuffled[idx].dropna().values\n",
    "    # random permutation\n",
    "    perm = np.random.permutation(len(T_year))\n",
    "    # assign to same positions (only where not nan)\n",
    "    T_year_perm = T_year[perm]\n",
    "    # fill back to T_shuffled; but we need to preserve nans; we will create a mapping\n",
    "    pos = np.where(idx)[0]\n",
    "    non_nan_idx = np.where(~df.loc[idx, 'vulnerability_lag1'].isna())[0]\n",
    "    for i, row_idx in enumerate(non_nan_idx):\n",
    "        T_shuffled.iloc[pos[row_idx]] = T_year_perm[i]\n",
    "\n",
    "# compute random assignment result\n",
    "res_rand = run_dml_binary(T_shuffled, df['high_spread'], X_base, df['year'], df['iso3c'])\n",
    "print('Random country assignment effect:', res_rand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
